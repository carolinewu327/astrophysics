{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34f28f2032c3ecd",
   "metadata": {},
   "source": [
    "### GALAXY PAIRS CATALOG"
   ]
  },
  {
   "cell_type": "code",
   "id": "2eb5e21b-7714-48ca-8895-c159e39c8a46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:53:09.236327Z",
     "start_time": "2025-11-09T14:53:09.228308Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "from astropy.cosmology import Planck18 as cosmo\n",
    "from tqdm.notebook import tqdm\n",
    "from helper import preprocess_catalog_galactic\n",
    "from helper import load_catalog\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "187bf040c99b7996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:55:13.711229Z",
     "start_time": "2025-11-09T14:55:12.120781Z"
    }
   },
   "source": [
    "# --- Settings ---\n",
    "dataset = \"BOSS\"\n",
    "if dataset == \"eBOSS\":\n",
    "    catalog, region = \"LRG\", \"NGC\"\n",
    "    real_file = f\"data/eBOSS/eBOSS_{catalog}_clustering_data-{region}-vDR16.fits\"\n",
    "    rand_file = f\"data/eBOSS/eBOSS_LRG_clustering_random-{region}-vDR16.fits\"\n",
    "elif dataset == \"BOSS\":\n",
    "    catalog, region = \"CMASS\", \"South\"\n",
    "    real_file = f\"data/BOSS/galaxy_DR12v5_CMASS_{region}.fits\"\n",
    "    rand_file = f\"data/BOSS/random0_DR12v5_CMASS_{region}.fits\"\n",
    "else:\n",
    "    raise ValueError(\"dataset must be eBOSS or BOSS\")\n",
    "\n",
    "alm_file = \"data/COM_Lensing_4096_R3.00/MV/dat_klm.fits\"\n",
    "mask_file = \"data/COM_Lensing_4096_R3.00/mask.fits\""
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "`newbyteorder` was removed from the ndarray class in NumPy 2.0. Use `arr.view(arr.dtype.newbyteorder(order))` instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 32\u001B[39m\n\u001B[32m     29\u001B[39m random_South  = \u001B[33m\"\u001B[39m\u001B[33mdata/BOSS/random0_DR12v5_CMASS_South.fits\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# Load and concatenate dataframes\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m df_N = \u001B[43mfits_to_df\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_North\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     33\u001B[39m df_S = fits_to_df(dataset_South)\n\u001B[32m     34\u001B[39m real_file = pd.concat([df_N, df_S], ignore_index=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 9\u001B[39m, in \u001B[36mfits_to_df\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# If it's a record array, convert directly\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(data, \u001B[33m'\u001B[39m\u001B[33mcolumns\u001B[39m\u001B[33m'\u001B[39m):\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m pd.DataFrame(\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbyteswap\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnewbyteorder\u001B[49m())\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# Otherwise, if it’s a normal ndarray (like shape (N, M)), handle manually\u001B[39;00m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m data.ndim == \u001B[32m2\u001B[39m:\n",
      "\u001B[31mAttributeError\u001B[39m: `newbyteorder` was removed from the ndarray class in NumPy 2.0. Use `arr.view(arr.dtype.newbyteorder(order))` instead."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "3c4c76300e148570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:53:10.536876Z",
     "start_time": "2025-11-06T01:19:47.211192Z"
    }
   },
   "source": [
    "def compute_angle_cosine(l1, b1, l2, b2):\n",
    "    \"\"\"\n",
    "    Compute cosine of angle θ between two directions (in degrees) in Galactic coordinates.\n",
    "    \"\"\"\n",
    "    l1_rad, b1_rad = np.radians(l1), np.radians(b1)\n",
    "    l2_rad, b2_rad = np.radians(l2), np.radians(b2)\n",
    "\n",
    "    cos_theta = (\n",
    "        np.cos(b1_rad) * np.cos(l1_rad) * np.cos(b2_rad) * np.cos(l2_rad) +\n",
    "        np.cos(b1_rad) * np.sin(l1_rad) * np.cos(b2_rad) * np.sin(l2_rad) +\n",
    "        np.sin(b1_rad) * np.sin(b2_rad)\n",
    "    )\n",
    "    return cos_theta\n",
    "\n",
    "def galactic_to_cartesian(l, b, D):\n",
    "    \"\"\"\n",
    "    Convert Galactic coordinates (l, b in degrees, D in Mpc/h) to 3D Cartesian.\n",
    "    \"\"\"\n",
    "    l_rad, b_rad = np.radians(l), np.radians(b)\n",
    "    x = D * np.cos(b_rad) * np.cos(l_rad)\n",
    "    y = D * np.cos(b_rad) * np.sin(l_rad)\n",
    "    z = D * np.sin(b_rad)\n",
    "    return np.column_stack([x, y, z])\n",
    "\n",
    "def create_distance_bins(D, r_par_max):\n",
    "    \"\"\"\n",
    "    Create bins for parallel distance (comoving distance) to limit search space.\n",
    "    \"\"\"\n",
    "    # Sort galaxies by distance and create bins\n",
    "    sorted_indices = np.argsort(D)\n",
    "    sorted_D = D[sorted_indices]\n",
    "    \n",
    "    # Create bins with width = r_par_max to limit parallel distance searches\n",
    "    bins = {}\n",
    "    current_bin = 0\n",
    "    bin_edges = [0]\n",
    "    \n",
    "    for i, d in enumerate(sorted_D):\n",
    "        if i == 0 or d - sorted_D[bin_edges[-1]] > r_par_max * 2:\n",
    "            bin_edges.append(i)\n",
    "            current_bin += 1\n",
    "    \n",
    "    bin_edges.append(len(D))\n",
    "    \n",
    "    # Store which galaxies are in searchable range for each bin\n",
    "    for i in range(len(bin_edges) - 1):\n",
    "        bin_start = bin_edges[i]\n",
    "        bin_end = bin_edges[i + 1]\n",
    "        bins[i] = sorted_indices[bin_start:bin_end]\n",
    "    \n",
    "    return bins, sorted_indices, sorted_D\n",
    "\n",
    "def process_chunk_optimized(chunk_start_end, l, b, D, data_filtered, weights_valid, z, \n",
    "                            r_par_max, r_perp_min, r_perp_max, sorted_indices, sorted_D):\n",
    "    \"\"\"\n",
    "    Process a chunk of galaxies using VECTORIZED distance-sorted approach.\n",
    "    This is MUCH faster than the loop version.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    chunk_start, chunk_end = chunk_start_end\n",
    "    \n",
    "    # Pre-allocate lists for vectorized operations\n",
    "    i_list, j_list = [], []\n",
    "    Dc1_list, Dc2_list = [], []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    last_print_time = start_time\n",
    "    \n",
    "    # First pass: collect candidate pairs using fast filtering\n",
    "    for idx in range(chunk_start, chunk_end):\n",
    "        i = sorted_indices[idx]\n",
    "        Dc1 = D[i]\n",
    "        \n",
    "        # Debug: print progress every 1000 galaxies\n",
    "        if (idx - chunk_start) > 0 and (idx - chunk_start) % 1000 == 0:\n",
    "            current_time = time.time()\n",
    "            elapsed = current_time - start_time\n",
    "            interval_time = current_time - last_print_time\n",
    "            galaxies_processed = idx - chunk_start\n",
    "            rate = 1000 / interval_time if interval_time > 0 else 0\n",
    "            pairs_found = len(i_list)\n",
    "            print(f\"  Chunk [{chunk_start}-{chunk_end}]: Processed {galaxies_processed}/{chunk_end-chunk_start} galaxies \"\n",
    "                  f\"({rate:.1f} gal/s, {interval_time:.1f}s for last 1000, {pairs_found} pairs so far)\")\n",
    "            last_print_time = current_time\n",
    "        \n",
    "        # Binary search for galaxies within r_par_max\n",
    "        left_bound = np.searchsorted(sorted_D, Dc1 - r_par_max, side='left')\n",
    "        right_bound = np.searchsorted(sorted_D, Dc1 + r_par_max, side='right')\n",
    "        search_start = max(idx + 1, left_bound)\n",
    "        search_end = right_bound\n",
    "        \n",
    "        if search_end <= search_start:\n",
    "            continue\n",
    "        \n",
    "        # Get all candidate j indices\n",
    "        j_candidates = sorted_indices[search_start:search_end]\n",
    "        Dc2_candidates = D[j_candidates]\n",
    "        \n",
    "        # Vectorized angular filtering\n",
    "        l1, b1 = l[i], b[i]\n",
    "        l2_candidates = l[j_candidates]\n",
    "        b2_candidates = b[j_candidates]\n",
    "        \n",
    "        # Fast approximate angular separation\n",
    "        dl = np.abs(l2_candidates - l1)\n",
    "        db = np.abs(b2_candidates - b1)\n",
    "        \n",
    "        # Handle wrapping around 360 degrees for longitude\n",
    "        dl = np.minimum(dl, 360 - dl)\n",
    "        \n",
    "        D_avg = (Dc1 + Dc2_candidates) / 2.0\n",
    "        min_angle = r_perp_min / D_avg\n",
    "        max_angle = r_perp_max / D_avg\n",
    "        \n",
    "        rough_angle = np.sqrt((np.radians(dl))**2 + (np.radians(db))**2)\n",
    "        \n",
    "        # Filter by approximate angle (with generous margins)\n",
    "        angle_mask = (rough_angle >= min_angle * 0.7) & (rough_angle <= max_angle * 1.3)\n",
    "        \n",
    "        if not np.any(angle_mask):\n",
    "            continue\n",
    "        \n",
    "        # Keep only candidates that pass angular filter\n",
    "        j_filtered = j_candidates[angle_mask]\n",
    "        \n",
    "        # Store for vectorized processing\n",
    "        i_list.extend([i] * len(j_filtered))\n",
    "        j_list.extend(j_filtered)\n",
    "    \n",
    "    if len(i_list) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Convert to arrays for vectorized computation\n",
    "    i_arr = np.array(i_list)\n",
    "    j_arr = np.array(j_list)\n",
    "    \n",
    "    # Vectorized precise angle calculation\n",
    "    l1_arr, b1_arr = l[i_arr], b[i_arr]\n",
    "    l2_arr, b2_arr = l[j_arr], b[j_arr]\n",
    "    \n",
    "    l1_rad, b1_rad = np.radians(l1_arr), np.radians(b1_arr)\n",
    "    l2_rad, b2_rad = np.radians(l2_arr), np.radians(b2_arr)\n",
    "    \n",
    "    cos_theta = (\n",
    "        np.cos(b1_rad) * np.cos(l1_rad) * np.cos(b2_rad) * np.cos(l2_rad) +\n",
    "        np.cos(b1_rad) * np.sin(l1_rad) * np.cos(b2_rad) * np.sin(l2_rad) +\n",
    "        np.sin(b1_rad) * np.sin(b2_rad)\n",
    "    )\n",
    "    \n",
    "    theta = np.arccos(np.clip(cos_theta, -1, 1))\n",
    "    \n",
    "    Dc1_arr = D[i_arr]\n",
    "    Dc2_arr = D[j_arr]\n",
    "    D_avg_arr = (Dc1_arr + Dc2_arr) / 2.0\n",
    "    r_perp_arr = D_avg_arr * theta\n",
    "    \n",
    "    # Final filter\n",
    "    final_mask = (r_perp_arr >= r_perp_min) & (r_perp_arr <= r_perp_max)\n",
    "    \n",
    "    # Build pairs for accepted candidates\n",
    "    i_final = i_arr[final_mask]\n",
    "    j_final = j_arr[final_mask]\n",
    "    \n",
    "    pairs = []\n",
    "    for idx in range(len(i_final)):\n",
    "        i = i_final[idx]\n",
    "        j = j_final[idx]\n",
    "        Dmid = cosmo.comoving_distance((z[i] + z[j]) / 2).value * cosmo.h\n",
    "        pairs.append({\n",
    "            'l1': l[i], 'b1': b[i], 'z1': z[i], 'w1': weights_valid[i], 'Dc1': D[i], 'ID1': data_filtered[i]['ID'],\n",
    "            'l2': l[j], 'b2': b[j], 'z2': z[j], 'w2': weights_valid[j], 'Dc2': D[j], 'ID2': data_filtered[j]['ID'],\n",
    "            'Dmid': Dmid\n",
    "        })\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"  Chunk [{chunk_start}-{chunk_end}] COMPLETE: {chunk_end-chunk_start} galaxies in {total_time:.1f}s, found {len(pairs)} pairs\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def build_galaxy_pair_catalog(data, weights, r_par_max=20, r_perp_min=18, r_perp_max=22, \n",
    "                               n_processes=None, chunk_size=10000, save_intermediate=True):\n",
    "    \"\"\"\n",
    "    Build galaxy pair catalog based on parallel and perpendicular distance criteria.\n",
    "    Uses distance sorting and VECTORIZED operations for maximum speed.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array\n",
    "        Galaxy catalog data\n",
    "    weights : array\n",
    "        Galaxy weights\n",
    "    r_par_max : float\n",
    "        Maximum parallel distance (Mpc/h)\n",
    "    r_perp_min : float\n",
    "        Minimum perpendicular distance (Mpc/h)\n",
    "    r_perp_max : float\n",
    "        Maximum perpendicular distance (Mpc/h)\n",
    "    n_processes : int or None\n",
    "        Number of processes to use (default: cpu_count() - 1)\n",
    "    chunk_size : int\n",
    "        Number of galaxies to process per chunk\n",
    "    save_intermediate : bool\n",
    "        Save results every 10 chunks to avoid losing progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pairs : DataFrame\n",
    "        Galaxy pair catalog\n",
    "    \"\"\"\n",
    "    l, b, D, data_filtered, weights_valid = preprocess_catalog_galactic(data, weights)\n",
    "    z = data_filtered['Z']\n",
    "    \n",
    "    n_galaxies = len(data_filtered)\n",
    "    print(f\"Building pair catalog for {n_galaxies} galaxies...\")\n",
    "    print(f\"Search parameters: r_par_max={r_par_max}, r_perp_min={r_perp_min}, r_perp_max={r_perp_max}\")\n",
    "    \n",
    "    # Sort galaxies by distance for efficient searching\n",
    "    print(\"Sorting galaxies by comoving distance...\")\n",
    "    sorted_indices = np.argsort(D)\n",
    "    sorted_D = D[sorted_indices]\n",
    "    \n",
    "    # Estimate search space\n",
    "    avg_D = np.median(D)\n",
    "    expected_pairs_per_galaxy = int((2 * r_par_max / avg_D) * n_galaxies * 0.01)\n",
    "    print(f\"Average distance: {avg_D:.1f} Mpc/h\")\n",
    "    print(f\"Estimated search space per galaxy: ~{expected_pairs_per_galaxy} candidates\")\n",
    "    \n",
    "    # Split into chunks for parallel processing\n",
    "    n_chunks = int(np.ceil(n_galaxies / chunk_size))\n",
    "    chunk_ranges = [(i * chunk_size, min((i + 1) * chunk_size, n_galaxies)) \n",
    "                    for i in range(n_chunks)]\n",
    "    \n",
    "    if n_processes is None:\n",
    "        # Use 75% of available cores (better for Windows and memory usage)\n",
    "        n_processes = max(1, int(cpu_count() * 0.75))\n",
    "    \n",
    "    # Cap at reasonable maximum (too many processes = overhead + memory issues)\n",
    "    n_processes = min(n_processes, 12)\n",
    "    \n",
    "    print(f\"Processing {n_chunks} chunks using {n_processes} processes...\")\n",
    "    print(f\"NOTE: With r_perp_min={r_perp_min}, you may find MANY pairs. Consider r_perp_min > 5-10.\")\n",
    "    \n",
    "    # Create partial function with fixed parameters\n",
    "    process_func = partial(\n",
    "        process_chunk_optimized,\n",
    "        l=l, b=b, D=D, \n",
    "        data_filtered=data_filtered, weights_valid=weights_valid, z=z,\n",
    "        r_par_max=r_par_max, r_perp_min=r_perp_min, r_perp_max=r_perp_max,\n",
    "        sorted_indices=sorted_indices, sorted_D=sorted_D\n",
    "    )\n",
    "    \n",
    "    # Process in parallel with intermediate saves\n",
    "    all_pairs = []\n",
    "    output_file = f\"data/paircatalogs/galaxy_pairs_catalog_CMASS_{region}_{r_par_max}_{r_perp_min}_{r_perp_max}hmpc.csv\"\n",
    "    \n",
    "    if n_processes > 1:\n",
    "        import time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"NOTE: Debug prints from worker processes won't appear here.\")\n",
    "        print(f\"Progress tracked by main process after each chunk completes.\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        chunk_start_time = time.time()\n",
    "        with Pool(processes=n_processes) as pool:\n",
    "            for i, result in enumerate(tqdm(\n",
    "                pool.imap(process_func, chunk_ranges),\n",
    "                total=len(chunk_ranges),\n",
    "                desc=\"Finding pairs\"\n",
    "            )):\n",
    "                chunk_time = time.time() - chunk_start_time\n",
    "                all_pairs.extend(result)\n",
    "                \n",
    "                # Print summary after each chunk completes\n",
    "                chunk_start, chunk_end = chunk_ranges[i]\n",
    "                n_pairs_in_chunk = len(result)\n",
    "                total_pairs_so_far = sum(len(p) if isinstance(p, list) else 1 for p in all_pairs)\n",
    "                print(f\"\\n  Chunk {i+1}/{len(chunk_ranges)} [{chunk_start}-{chunk_end}]: \"\n",
    "                      f\"Found {n_pairs_in_chunk:,} pairs in {chunk_time:.1f}s \"\n",
    "                      f\"({(chunk_end-chunk_start)/chunk_time:.1f} gal/s) | \"\n",
    "                      f\"Total: {total_pairs_so_far:,} pairs\")\n",
    "                \n",
    "                # Save intermediate results every 10 chunks\n",
    "                if save_intermediate and (i + 1) % 10 == 0:\n",
    "                    temp_df = pd.DataFrame([p for chunk in all_pairs for p in [chunk]] if isinstance(all_pairs[0], dict) else all_pairs)\n",
    "                    temp_file = output_file.replace('.csv', f'_temp_{i+1}.csv')\n",
    "                    temp_df.to_csv(temp_file, index=False)\n",
    "                    print(f\"  ✓ Checkpoint: Saved {len(temp_df):,} pairs to {temp_file}\")\n",
    "                \n",
    "                chunk_start_time = time.time()\n",
    "    else:\n",
    "        # Single process mode - debug prints WILL appear\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Running in SINGLE PROCESS mode - all debug prints visible.\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for i, chunk in enumerate(tqdm(chunk_ranges, desc=\"Finding pairs\")):\n",
    "            result = process_func(chunk)\n",
    "            all_pairs.extend(result)\n",
    "            \n",
    "            if save_intermediate and (i + 1) % 10 == 0:\n",
    "                temp_df = pd.DataFrame(all_pairs)\n",
    "                temp_file = output_file.replace('.csv', f'_temp_{i+1}.csv')\n",
    "                temp_df.to_csv(temp_file, index=False)\n",
    "                print(f\"\\n  Checkpoint: Saved {len(temp_df)} pairs to {temp_file}\")\n",
    "    \n",
    "    # Flatten and convert to DataFrame\n",
    "    pairs = [pair for chunk_pairs in all_pairs for pair in chunk_pairs] if all_pairs and isinstance(all_pairs[0], list) else all_pairs\n",
    "    \n",
    "    print(f\"\\nFound {len(pairs)} total pairs\")\n",
    "    \n",
    "    pairs_df = pd.DataFrame(pairs)\n",
    "    pairs_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved to {output_file}\")\n",
    "    \n",
    "    return pairs_df"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "3c4d6830-3e98-4a81-b878-7d89a4e85f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:53:10.545861Z",
     "start_time": "2025-11-06T01:19:47.235072Z"
    }
   },
   "source": [
    "# --- Run all ---\n",
    "if dataset == \"BOSS\":\n",
    "    z_min = .4\n",
    "    z_max = .7\n",
    "    weight = \"CMASS\"\n",
    "else: # eBOSS\n",
    "    z_min = 0\n",
    "    z_max = 10000\n",
    "    weight = True\n",
    "\n",
    "data_real, w_real = load_catalog(real_file, weights=weight, z_min=z_min, z_max=z_max)\n",
    "# data_rand, w_rand = load_catalog(rand_file, random_fraction=0.10)\n",
    "\n",
    "# Build galaxy pair catalog with VECTORIZED version\n",
    "# This eliminates inner Python loops for massive speedup\n",
    "pair_catalog = build_galaxy_pair_catalog(\n",
    "    data_real, w_real, \n",
    "    r_par_max=10,\n",
    "    r_perp_min=9,          # NOTE: r_perp_min=0 will find MANY pairs! Consider 5-10 for faster results\n",
    "    r_perp_max=11,\n",
    "    n_processes=1,      # Auto: uses 75% of cores (capped at 12). On 16-core: will use 12\n",
    "    chunk_size=10000,      # Even larger chunks with vectorization\n",
    "    save_intermediate=True # Saves progress every 10 chunks\n",
    ")\n",
    "print(f\"Total valid pairs: {len(pair_catalog)}\")\n",
    "\n",
    "# Jackknife real galaxies\n",
    "# kappa_real, sigma_real = jackknife_stack_healpix(data_real, w_real, data_rand, nside=10)\n",
    "# sn_real = np.zeros_like(kappa_real)\n",
    "# valid = sigma_real > 0\n",
    "# sn_real[valid] = kappa_real[valid] / sigma_real[valid]\n",
    "# kappa_rand, sigma_rand, sn_rand = stack_kappa(data_rand, w_rand, \"Random\")\n",
    "# kappa_sub = kappa_real - kappa_rand\n",
    "# kappa_smooth = gaussian_filter(kappa_sub, sigma=2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building pair catalog for 213205 galaxies...\n",
      "Search parameters: r_par_max=10, r_perp_min=9, r_perp_max=11\n",
      "Sorting galaxies by comoving distance...\n",
      "Average distance: 1406.6 Mpc/h\n",
      "Estimated search space per galaxy: ~30 candidates\n",
      "Processing 22 chunks using 1 processes...\n",
      "NOTE: With r_perp_min=9, you may find MANY pairs. Consider r_perp_min > 5-10.\n",
      "\n",
      "============================================================\n",
      "Running in SINGLE PROCESS mode - all debug prints visible.\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Finding pairs:   0%|          | 0/22 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b021378ee8044f09a4a0452d88b33791"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chunk [0-10000]: Processed 1000/10000 galaxies (28059.1 gal/s, 0.0s for last 1000, 326 pairs so far)\n",
      "  Chunk [0-10000]: Processed 2000/10000 galaxies (39436.8 gal/s, 0.0s for last 1000, 704 pairs so far)\n",
      "  Chunk [0-10000]: Processed 3000/10000 galaxies (37574.3 gal/s, 0.0s for last 1000, 1174 pairs so far)\n",
      "  Chunk [0-10000]: Processed 4000/10000 galaxies (42988.5 gal/s, 0.0s for last 1000, 1710 pairs so far)\n",
      "  Chunk [0-10000]: Processed 5000/10000 galaxies (41379.0 gal/s, 0.0s for last 1000, 2356 pairs so far)\n",
      "  Chunk [0-10000]: Processed 6000/10000 galaxies (33228.0 gal/s, 0.0s for last 1000, 3224 pairs so far)\n",
      "  Chunk [0-10000]: Processed 7000/10000 galaxies (30820.5 gal/s, 0.0s for last 1000, 4167 pairs so far)\n",
      "  Chunk [0-10000]: Processed 8000/10000 galaxies (31922.3 gal/s, 0.0s for last 1000, 5176 pairs so far)\n",
      "  Chunk [0-10000]: Processed 9000/10000 galaxies (28280.7 gal/s, 0.0s for last 1000, 6474 pairs so far)\n",
      "  Chunk [0-10000] COMPLETE: 10000 galaxies in 0.4s, found 1948 pairs\n",
      "  Chunk [10000-20000]: Processed 1000/10000 galaxies (26300.0 gal/s, 0.0s for last 1000, 1311 pairs so far)\n",
      "  Chunk [10000-20000]: Processed 2000/10000 galaxies (28761.0 gal/s, 0.0s for last 1000, 2785 pairs so far)\n",
      "  Chunk [10000-20000]: Processed 3000/10000 galaxies (24778.9 gal/s, 0.0s for last 1000, 4418 pairs so far)\n",
      "  Chunk [10000-20000]: Processed 4000/10000 galaxies (26130.1 gal/s, 0.0s for last 1000, 6141 pairs so far)\n",
      "  Chunk [10000-20000]: Processed 5000/10000 galaxies (25789.2 gal/s, 0.0s for last 1000, 7868 pairs so far)\n",
      "  Chunk [10000-20000]: Processed 6000/10000 galaxies (22852.4 gal/s, 0.0s for last 1000, 9802 pairs so far)\n",
      "  Chunk [10000-20000]: Processed 7000/10000 galaxies (24577.4 gal/s, 0.0s for last 1000, 11822 pairs so far)\n",
      "  Chunk [10000-20000]: Processed 8000/10000 galaxies (23044.6 gal/s, 0.0s for last 1000, 13728 pairs so far)\n",
      "  Chunk [10000-20000]: Processed 9000/10000 galaxies (21414.7 gal/s, 0.0s for last 1000, 15737 pairs so far)\n",
      "  Chunk [10000-20000] COMPLETE: 10000 galaxies in 0.6s, found 4439 pairs\n",
      "  Chunk [20000-30000]: Processed 1000/10000 galaxies (21588.9 gal/s, 0.0s for last 1000, 2108 pairs so far)\n",
      "  Chunk [20000-30000]: Processed 2000/10000 galaxies (23379.2 gal/s, 0.0s for last 1000, 4046 pairs so far)\n",
      "  Chunk [20000-30000]: Processed 3000/10000 galaxies (23087.9 gal/s, 0.0s for last 1000, 5964 pairs so far)\n",
      "  Chunk [20000-30000]: Processed 4000/10000 galaxies (22685.4 gal/s, 0.0s for last 1000, 7995 pairs so far)\n",
      "  Chunk [20000-30000]: Processed 5000/10000 galaxies (21492.4 gal/s, 0.0s for last 1000, 10029 pairs so far)\n",
      "  Chunk [20000-30000]: Processed 6000/10000 galaxies (21012.5 gal/s, 0.0s for last 1000, 11918 pairs so far)\n",
      "  Chunk [20000-30000]: Processed 7000/10000 galaxies (20692.8 gal/s, 0.0s for last 1000, 14160 pairs so far)\n",
      "  Chunk [20000-30000]: Processed 8000/10000 galaxies (20386.9 gal/s, 0.0s for last 1000, 16238 pairs so far)\n",
      "  Chunk [20000-30000]: Processed 9000/10000 galaxies (21116.7 gal/s, 0.0s for last 1000, 18241 pairs so far)\n",
      "  Chunk [20000-30000] COMPLETE: 10000 galaxies in 0.6s, found 5093 pairs\n",
      "  Chunk [30000-40000]: Processed 1000/10000 galaxies (19907.3 gal/s, 0.1s for last 1000, 2351 pairs so far)\n",
      "  Chunk [30000-40000]: Processed 2000/10000 galaxies (20488.4 gal/s, 0.0s for last 1000, 4493 pairs so far)\n",
      "  Chunk [30000-40000]: Processed 3000/10000 galaxies (16182.9 gal/s, 0.1s for last 1000, 6557 pairs so far)\n",
      "  Chunk [30000-40000]: Processed 4000/10000 galaxies (12869.2 gal/s, 0.1s for last 1000, 8850 pairs so far)\n",
      "  Chunk [30000-40000]: Processed 5000/10000 galaxies (11908.4 gal/s, 0.1s for last 1000, 11051 pairs so far)\n",
      "  Chunk [30000-40000]: Processed 6000/10000 galaxies (17934.3 gal/s, 0.1s for last 1000, 13478 pairs so far)\n",
      "  Chunk [30000-40000]: Processed 7000/10000 galaxies (17883.4 gal/s, 0.1s for last 1000, 15793 pairs so far)\n",
      "  Chunk [30000-40000]: Processed 8000/10000 galaxies (15977.3 gal/s, 0.1s for last 1000, 18085 pairs so far)\n",
      "  Chunk [30000-40000]: Processed 9000/10000 galaxies (18219.5 gal/s, 0.1s for last 1000, 20523 pairs so far)\n",
      "  Chunk [30000-40000] COMPLETE: 10000 galaxies in 0.8s, found 5818 pairs\n",
      "  Chunk [40000-50000]: Processed 1000/10000 galaxies (17519.0 gal/s, 0.1s for last 1000, 2437 pairs so far)\n",
      "  Chunk [40000-50000]: Processed 2000/10000 galaxies (17758.0 gal/s, 0.1s for last 1000, 4801 pairs so far)\n",
      "  Chunk [40000-50000]: Processed 3000/10000 galaxies (18053.8 gal/s, 0.1s for last 1000, 7303 pairs so far)\n",
      "  Chunk [40000-50000]: Processed 4000/10000 galaxies (16466.5 gal/s, 0.1s for last 1000, 9703 pairs so far)\n",
      "  Chunk [40000-50000]: Processed 5000/10000 galaxies (18222.6 gal/s, 0.1s for last 1000, 12107 pairs so far)\n",
      "  Chunk [40000-50000]: Processed 6000/10000 galaxies (17576.5 gal/s, 0.1s for last 1000, 14504 pairs so far)\n",
      "  Chunk [40000-50000]: Processed 7000/10000 galaxies (18432.9 gal/s, 0.1s for last 1000, 16972 pairs so far)\n",
      "  Chunk [40000-50000]: Processed 8000/10000 galaxies (17217.9 gal/s, 0.1s for last 1000, 19236 pairs so far)\n",
      "  Chunk [40000-50000]: Processed 9000/10000 galaxies (17784.5 gal/s, 0.1s for last 1000, 21796 pairs so far)\n",
      "  Chunk [40000-50000] COMPLETE: 10000 galaxies in 0.8s, found 5995 pairs\n",
      "  Chunk [50000-60000]: Processed 1000/10000 galaxies (15777.6 gal/s, 0.1s for last 1000, 2348 pairs so far)\n",
      "  Chunk [50000-60000]: Processed 2000/10000 galaxies (17488.4 gal/s, 0.1s for last 1000, 4858 pairs so far)\n",
      "  Chunk [50000-60000]: Processed 3000/10000 galaxies (16904.4 gal/s, 0.1s for last 1000, 7329 pairs so far)\n",
      "  Chunk [50000-60000]: Processed 4000/10000 galaxies (15795.1 gal/s, 0.1s for last 1000, 9752 pairs so far)\n",
      "  Chunk [50000-60000]: Processed 5000/10000 galaxies (17961.0 gal/s, 0.1s for last 1000, 12041 pairs so far)\n",
      "  Chunk [50000-60000]: Processed 6000/10000 galaxies (17795.9 gal/s, 0.1s for last 1000, 14471 pairs so far)\n",
      "  Chunk [50000-60000]: Processed 7000/10000 galaxies (17894.9 gal/s, 0.1s for last 1000, 16768 pairs so far)\n",
      "  Chunk [50000-60000]: Processed 8000/10000 galaxies (15444.3 gal/s, 0.1s for last 1000, 19118 pairs so far)\n",
      "  Chunk [50000-60000]: Processed 9000/10000 galaxies (17012.3 gal/s, 0.1s for last 1000, 21554 pairs so far)\n",
      "  Chunk [50000-60000] COMPLETE: 10000 galaxies in 0.8s, found 6123 pairs\n",
      "  Chunk [60000-70000]: Processed 1000/10000 galaxies (16207.4 gal/s, 0.1s for last 1000, 2296 pairs so far)\n",
      "  Chunk [60000-70000]: Processed 2000/10000 galaxies (17343.9 gal/s, 0.1s for last 1000, 4576 pairs so far)\n",
      "  Chunk [60000-70000]: Processed 3000/10000 galaxies (17820.9 gal/s, 0.1s for last 1000, 6875 pairs so far)\n",
      "  Chunk [60000-70000]: Processed 4000/10000 galaxies (15510.8 gal/s, 0.1s for last 1000, 8998 pairs so far)\n",
      "  Chunk [60000-70000]: Processed 5000/10000 galaxies (18109.0 gal/s, 0.1s for last 1000, 11364 pairs so far)\n",
      "  Chunk [60000-70000]: Processed 6000/10000 galaxies (17255.7 gal/s, 0.1s for last 1000, 13551 pairs so far)\n",
      "  Chunk [60000-70000]: Processed 7000/10000 galaxies (15812.3 gal/s, 0.1s for last 1000, 15785 pairs so far)\n",
      "  Chunk [60000-70000]: Processed 8000/10000 galaxies (15194.3 gal/s, 0.1s for last 1000, 18182 pairs so far)\n",
      "  Chunk [60000-70000]: Processed 9000/10000 galaxies (18085.8 gal/s, 0.1s for last 1000, 20537 pairs so far)\n",
      "  Chunk [60000-70000] COMPLETE: 10000 galaxies in 0.8s, found 5694 pairs\n",
      "  Chunk [70000-80000]: Processed 1000/10000 galaxies (15195.3 gal/s, 0.1s for last 1000, 2497 pairs so far)\n",
      "  Chunk [70000-80000]: Processed 2000/10000 galaxies (17227.4 gal/s, 0.1s for last 1000, 4757 pairs so far)\n",
      "  Chunk [70000-80000]: Processed 3000/10000 galaxies (18292.6 gal/s, 0.1s for last 1000, 6959 pairs so far)\n",
      "  Chunk [70000-80000]: Processed 4000/10000 galaxies (16669.7 gal/s, 0.1s for last 1000, 9033 pairs so far)\n",
      "  Chunk [70000-80000]: Processed 5000/10000 galaxies (17998.9 gal/s, 0.1s for last 1000, 11329 pairs so far)\n",
      "  Chunk [70000-80000]: Processed 6000/10000 galaxies (16951.1 gal/s, 0.1s for last 1000, 13613 pairs so far)\n",
      "  Chunk [70000-80000]: Processed 7000/10000 galaxies (18168.6 gal/s, 0.1s for last 1000, 15803 pairs so far)\n",
      "  Chunk [70000-80000]: Processed 8000/10000 galaxies (16055.7 gal/s, 0.1s for last 1000, 17957 pairs so far)\n",
      "  Chunk [70000-80000]: Processed 9000/10000 galaxies (18287.6 gal/s, 0.1s for last 1000, 20222 pairs so far)\n",
      "  Chunk [70000-80000] COMPLETE: 10000 galaxies in 0.8s, found 5624 pairs\n",
      "  Chunk [80000-90000]: Processed 1000/10000 galaxies (16716.4 gal/s, 0.1s for last 1000, 2303 pairs so far)\n",
      "  Chunk [80000-90000]: Processed 2000/10000 galaxies (17979.4 gal/s, 0.1s for last 1000, 4639 pairs so far)\n",
      "  Chunk [80000-90000]: Processed 3000/10000 galaxies (17784.8 gal/s, 0.1s for last 1000, 6771 pairs so far)\n",
      "  Chunk [80000-90000]: Processed 4000/10000 galaxies (15910.1 gal/s, 0.1s for last 1000, 9187 pairs so far)\n",
      "  Chunk [80000-90000]: Processed 5000/10000 galaxies (17462.7 gal/s, 0.1s for last 1000, 11461 pairs so far)\n",
      "  Chunk [80000-90000]: Processed 6000/10000 galaxies (17421.6 gal/s, 0.1s for last 1000, 13684 pairs so far)\n",
      "  Chunk [80000-90000]: Processed 7000/10000 galaxies (17555.3 gal/s, 0.1s for last 1000, 15969 pairs so far)\n",
      "  Chunk [80000-90000]: Processed 8000/10000 galaxies (16445.5 gal/s, 0.1s for last 1000, 18152 pairs so far)\n",
      "  Chunk [80000-90000]: Processed 9000/10000 galaxies (17922.4 gal/s, 0.1s for last 1000, 20357 pairs so far)\n",
      "  Chunk [80000-90000] COMPLETE: 10000 galaxies in 0.8s, found 5530 pairs\n",
      "  Chunk [90000-100000]: Processed 1000/10000 galaxies (17292.6 gal/s, 0.1s for last 1000, 2170 pairs so far)\n",
      "  Chunk [90000-100000]: Processed 2000/10000 galaxies (18251.6 gal/s, 0.1s for last 1000, 4346 pairs so far)\n",
      "  Chunk [90000-100000]: Processed 3000/10000 galaxies (17558.9 gal/s, 0.1s for last 1000, 6448 pairs so far)\n",
      "  Chunk [90000-100000]: Processed 4000/10000 galaxies (15905.8 gal/s, 0.1s for last 1000, 8585 pairs so far)\n",
      "  Chunk [90000-100000]: Processed 5000/10000 galaxies (16814.3 gal/s, 0.1s for last 1000, 10659 pairs so far)\n",
      "  Chunk [90000-100000]: Processed 6000/10000 galaxies (17814.2 gal/s, 0.1s for last 1000, 12742 pairs so far)\n",
      "  Chunk [90000-100000]: Processed 7000/10000 galaxies (17624.9 gal/s, 0.1s for last 1000, 14980 pairs so far)\n",
      "  Chunk [90000-100000]: Processed 8000/10000 galaxies (16605.8 gal/s, 0.1s for last 1000, 17032 pairs so far)\n",
      "  Chunk [90000-100000]: Processed 9000/10000 galaxies (18062.6 gal/s, 0.1s for last 1000, 19075 pairs so far)\n",
      "  Chunk [90000-100000] COMPLETE: 10000 galaxies in 0.8s, found 5269 pairs\n",
      "\n",
      "  Checkpoint: Saved 51533 pairs to data/paircatalogs/galaxy_pairs_catalog_CMASS_South_10_9_11hmpc_temp_10.csv\n",
      "  Chunk [100000-110000]: Processed 1000/10000 galaxies (16635.6 gal/s, 0.1s for last 1000, 1990 pairs so far)\n",
      "  Chunk [100000-110000]: Processed 2000/10000 galaxies (10420.8 gal/s, 0.1s for last 1000, 3831 pairs so far)\n",
      "  Chunk [100000-110000]: Processed 3000/10000 galaxies (12243.4 gal/s, 0.1s for last 1000, 5703 pairs so far)\n",
      "  Chunk [100000-110000]: Processed 4000/10000 galaxies (17306.4 gal/s, 0.1s for last 1000, 7655 pairs so far)\n",
      "  Chunk [100000-110000]: Processed 5000/10000 galaxies (18047.3 gal/s, 0.1s for last 1000, 9740 pairs so far)\n",
      "  Chunk [100000-110000]: Processed 6000/10000 galaxies (17670.6 gal/s, 0.1s for last 1000, 11896 pairs so far)\n",
      "  Chunk [100000-110000]: Processed 7000/10000 galaxies (16637.3 gal/s, 0.1s for last 1000, 13980 pairs so far)\n",
      "  Chunk [100000-110000]: Processed 8000/10000 galaxies (16553.0 gal/s, 0.1s for last 1000, 16130 pairs so far)\n",
      "  Chunk [100000-110000]: Processed 9000/10000 galaxies (17415.5 gal/s, 0.1s for last 1000, 18213 pairs so far)\n",
      "  Chunk [100000-110000] COMPLETE: 10000 galaxies in 0.8s, found 5142 pairs\n",
      "  Chunk [110000-120000]: Processed 1000/10000 galaxies (12432.3 gal/s, 0.1s for last 1000, 1979 pairs so far)\n",
      "  Chunk [110000-120000]: Processed 2000/10000 galaxies (17972.9 gal/s, 0.1s for last 1000, 3850 pairs so far)\n",
      "  Chunk [110000-120000]: Processed 3000/10000 galaxies (18265.5 gal/s, 0.1s for last 1000, 5895 pairs so far)\n",
      "  Chunk [110000-120000]: Processed 4000/10000 galaxies (15878.1 gal/s, 0.1s for last 1000, 7998 pairs so far)\n",
      "  Chunk [110000-120000]: Processed 5000/10000 galaxies (18131.7 gal/s, 0.1s for last 1000, 10075 pairs so far)\n",
      "  Chunk [110000-120000]: Processed 6000/10000 galaxies (17804.7 gal/s, 0.1s for last 1000, 12170 pairs so far)\n",
      "  Chunk [110000-120000]: Processed 7000/10000 galaxies (15446.4 gal/s, 0.1s for last 1000, 14120 pairs so far)\n",
      "  Chunk [110000-120000]: Processed 8000/10000 galaxies (8749.8 gal/s, 0.1s for last 1000, 16058 pairs so far)\n",
      "  Chunk [110000-120000]: Processed 9000/10000 galaxies (16037.9 gal/s, 0.1s for last 1000, 18010 pairs so far)\n",
      "  Chunk [110000-120000] COMPLETE: 10000 galaxies in 0.8s, found 4801 pairs\n",
      "  Chunk [120000-130000]: Processed 1000/10000 galaxies (16345.5 gal/s, 0.1s for last 1000, 2043 pairs so far)\n",
      "  Chunk [120000-130000]: Processed 2000/10000 galaxies (15828.3 gal/s, 0.1s for last 1000, 4151 pairs so far)\n",
      "  Chunk [120000-130000]: Processed 3000/10000 galaxies (15882.9 gal/s, 0.1s for last 1000, 6120 pairs so far)\n",
      "  Chunk [120000-130000]: Processed 4000/10000 galaxies (15638.7 gal/s, 0.1s for last 1000, 8168 pairs so far)\n",
      "  Chunk [120000-130000]: Processed 5000/10000 galaxies (17826.6 gal/s, 0.1s for last 1000, 10034 pairs so far)\n",
      "  Chunk [120000-130000]: Processed 6000/10000 galaxies (17865.1 gal/s, 0.1s for last 1000, 11985 pairs so far)\n",
      "  Chunk [120000-130000]: Processed 7000/10000 galaxies (17943.0 gal/s, 0.1s for last 1000, 13922 pairs so far)\n",
      "  Chunk [120000-130000]: Processed 8000/10000 galaxies (16777.4 gal/s, 0.1s for last 1000, 15713 pairs so far)\n",
      "  Chunk [120000-130000]: Processed 9000/10000 galaxies (18219.9 gal/s, 0.1s for last 1000, 17498 pairs so far)\n",
      "  Chunk [120000-130000] COMPLETE: 10000 galaxies in 0.7s, found 4635 pairs\n",
      "  Chunk [130000-140000]: Processed 1000/10000 galaxies (18515.1 gal/s, 0.1s for last 1000, 1649 pairs so far)\n",
      "  Chunk [130000-140000]: Processed 2000/10000 galaxies (14636.5 gal/s, 0.1s for last 1000, 3341 pairs so far)\n",
      "  Chunk [130000-140000]: Processed 3000/10000 galaxies (15608.4 gal/s, 0.1s for last 1000, 5020 pairs so far)\n",
      "  Chunk [130000-140000]: Processed 4000/10000 galaxies (9769.9 gal/s, 0.1s for last 1000, 7009 pairs so far)\n",
      "  Chunk [130000-140000]: Processed 5000/10000 galaxies (16191.4 gal/s, 0.1s for last 1000, 8884 pairs so far)\n",
      "  Chunk [130000-140000]: Processed 6000/10000 galaxies (18163.1 gal/s, 0.1s for last 1000, 10656 pairs so far)\n",
      "  Chunk [130000-140000]: Processed 7000/10000 galaxies (17850.1 gal/s, 0.1s for last 1000, 12399 pairs so far)\n",
      "  Chunk [130000-140000]: Processed 8000/10000 galaxies (14070.6 gal/s, 0.1s for last 1000, 14149 pairs so far)\n",
      "  Chunk [130000-140000]: Processed 9000/10000 galaxies (12145.2 gal/s, 0.1s for last 1000, 15974 pairs so far)\n",
      "  Chunk [130000-140000] COMPLETE: 10000 galaxies in 0.9s, found 4386 pairs\n",
      "  Chunk [140000-150000]: Processed 1000/10000 galaxies (15071.8 gal/s, 0.1s for last 1000, 1739 pairs so far)\n",
      "  Chunk [140000-150000]: Processed 2000/10000 galaxies (18715.7 gal/s, 0.1s for last 1000, 3592 pairs so far)\n",
      "  Chunk [140000-150000]: Processed 3000/10000 galaxies (17884.9 gal/s, 0.1s for last 1000, 5443 pairs so far)\n",
      "  Chunk [140000-150000]: Processed 4000/10000 galaxies (18131.7 gal/s, 0.1s for last 1000, 7007 pairs so far)\n",
      "  Chunk [140000-150000]: Processed 5000/10000 galaxies (19511.4 gal/s, 0.1s for last 1000, 8668 pairs so far)\n",
      "  Chunk [140000-150000]: Processed 6000/10000 galaxies (19148.2 gal/s, 0.1s for last 1000, 10252 pairs so far)\n",
      "  Chunk [140000-150000]: Processed 7000/10000 galaxies (17825.4 gal/s, 0.1s for last 1000, 11847 pairs so far)\n",
      "  Chunk [140000-150000]: Processed 8000/10000 galaxies (19515.7 gal/s, 0.1s for last 1000, 13464 pairs so far)\n",
      "  Chunk [140000-150000]: Processed 9000/10000 galaxies (19075.2 gal/s, 0.1s for last 1000, 15016 pairs so far)\n",
      "  Chunk [140000-150000] COMPLETE: 10000 galaxies in 0.7s, found 4006 pairs\n",
      "  Chunk [150000-160000]: Processed 1000/10000 galaxies (18657.7 gal/s, 0.1s for last 1000, 1669 pairs so far)\n",
      "  Chunk [150000-160000]: Processed 2000/10000 galaxies (21409.7 gal/s, 0.0s for last 1000, 3213 pairs so far)\n",
      "  Chunk [150000-160000]: Processed 3000/10000 galaxies (21643.1 gal/s, 0.0s for last 1000, 4740 pairs so far)\n",
      "  Chunk [150000-160000]: Processed 4000/10000 galaxies (21771.9 gal/s, 0.0s for last 1000, 6110 pairs so far)\n",
      "  Chunk [150000-160000]: Processed 5000/10000 galaxies (19893.4 gal/s, 0.1s for last 1000, 7659 pairs so far)\n",
      "  Chunk [150000-160000]: Processed 6000/10000 galaxies (18129.1 gal/s, 0.1s for last 1000, 9151 pairs so far)\n",
      "  Chunk [150000-160000]: Processed 7000/10000 galaxies (19694.3 gal/s, 0.1s for last 1000, 10837 pairs so far)\n",
      "  Chunk [150000-160000]: Processed 8000/10000 galaxies (19976.4 gal/s, 0.1s for last 1000, 12437 pairs so far)\n",
      "  Chunk [150000-160000]: Processed 9000/10000 galaxies (20524.6 gal/s, 0.0s for last 1000, 13920 pairs so far)\n",
      "  Chunk [150000-160000] COMPLETE: 10000 galaxies in 0.6s, found 3832 pairs\n",
      "  Chunk [160000-170000]: Processed 1000/10000 galaxies (17737.4 gal/s, 0.1s for last 1000, 1438 pairs so far)\n",
      "  Chunk [160000-170000]: Processed 2000/10000 galaxies (21934.1 gal/s, 0.0s for last 1000, 2828 pairs so far)\n",
      "  Chunk [160000-170000]: Processed 3000/10000 galaxies (21840.3 gal/s, 0.0s for last 1000, 4086 pairs so far)\n",
      "  Chunk [160000-170000]: Processed 4000/10000 galaxies (22301.5 gal/s, 0.0s for last 1000, 5377 pairs so far)\n",
      "  Chunk [160000-170000]: Processed 5000/10000 galaxies (22278.3 gal/s, 0.0s for last 1000, 6742 pairs so far)\n",
      "  Chunk [160000-170000]: Processed 6000/10000 galaxies (16461.2 gal/s, 0.1s for last 1000, 8106 pairs so far)\n",
      "  Chunk [160000-170000]: Processed 7000/10000 galaxies (22697.8 gal/s, 0.0s for last 1000, 9333 pairs so far)\n",
      "  Chunk [160000-170000]: Processed 8000/10000 galaxies (22862.5 gal/s, 0.0s for last 1000, 10626 pairs so far)\n",
      "  Chunk [160000-170000]: Processed 9000/10000 galaxies (23027.0 gal/s, 0.0s for last 1000, 11905 pairs so far)\n",
      "  Chunk [160000-170000] COMPLETE: 10000 galaxies in 0.6s, found 3218 pairs\n",
      "  Chunk [170000-180000]: Processed 1000/10000 galaxies (21199.1 gal/s, 0.0s for last 1000, 1149 pairs so far)\n",
      "  Chunk [170000-180000]: Processed 2000/10000 galaxies (19968.3 gal/s, 0.1s for last 1000, 2231 pairs so far)\n",
      "  Chunk [170000-180000]: Processed 3000/10000 galaxies (23275.3 gal/s, 0.0s for last 1000, 3317 pairs so far)\n",
      "  Chunk [170000-180000]: Processed 4000/10000 galaxies (23927.0 gal/s, 0.0s for last 1000, 4445 pairs so far)\n",
      "  Chunk [170000-180000]: Processed 5000/10000 galaxies (21306.6 gal/s, 0.0s for last 1000, 5611 pairs so far)\n",
      "  Chunk [170000-180000]: Processed 6000/10000 galaxies (23447.2 gal/s, 0.0s for last 1000, 6860 pairs so far)\n",
      "  Chunk [170000-180000]: Processed 7000/10000 galaxies (23895.9 gal/s, 0.0s for last 1000, 8079 pairs so far)\n",
      "  Chunk [170000-180000]: Processed 8000/10000 galaxies (24470.9 gal/s, 0.0s for last 1000, 9229 pairs so far)\n",
      "  Chunk [170000-180000]: Processed 9000/10000 galaxies (24789.3 gal/s, 0.0s for last 1000, 10353 pairs so far)\n",
      "  Chunk [170000-180000] COMPLETE: 10000 galaxies in 0.5s, found 2788 pairs\n",
      "  Chunk [180000-190000]: Processed 1000/10000 galaxies (23734.8 gal/s, 0.0s for last 1000, 1098 pairs so far)\n",
      "  Chunk [180000-190000]: Processed 2000/10000 galaxies (25612.0 gal/s, 0.0s for last 1000, 2114 pairs so far)\n",
      "  Chunk [180000-190000]: Processed 3000/10000 galaxies (25988.5 gal/s, 0.0s for last 1000, 3069 pairs so far)\n",
      "  Chunk [180000-190000]: Processed 4000/10000 galaxies (26415.8 gal/s, 0.0s for last 1000, 4134 pairs so far)\n",
      "  Chunk [180000-190000]: Processed 5000/10000 galaxies (26937.3 gal/s, 0.0s for last 1000, 5158 pairs so far)\n",
      "  Chunk [180000-190000]: Processed 6000/10000 galaxies (25858.0 gal/s, 0.0s for last 1000, 6065 pairs so far)\n",
      "  Chunk [180000-190000]: Processed 7000/10000 galaxies (27219.7 gal/s, 0.0s for last 1000, 6990 pairs so far)\n",
      "  Chunk [180000-190000]: Processed 8000/10000 galaxies (28188.9 gal/s, 0.0s for last 1000, 7889 pairs so far)\n",
      "  Chunk [180000-190000]: Processed 9000/10000 galaxies (28545.3 gal/s, 0.0s for last 1000, 8755 pairs so far)\n",
      "  Chunk [180000-190000] COMPLETE: 10000 galaxies in 0.5s, found 2450 pairs\n",
      "  Chunk [190000-200000]: Processed 1000/10000 galaxies (25438.8 gal/s, 0.0s for last 1000, 832 pairs so far)\n",
      "  Chunk [190000-200000]: Processed 2000/10000 galaxies (29759.1 gal/s, 0.0s for last 1000, 1631 pairs so far)\n",
      "  Chunk [190000-200000]: Processed 3000/10000 galaxies (20941.2 gal/s, 0.0s for last 1000, 2367 pairs so far)\n",
      "  Chunk [190000-200000]: Processed 4000/10000 galaxies (17016.1 gal/s, 0.1s for last 1000, 3098 pairs so far)\n",
      "  Chunk [190000-200000]: Processed 5000/10000 galaxies (17429.8 gal/s, 0.1s for last 1000, 3756 pairs so far)\n",
      "  Chunk [190000-200000]: Processed 6000/10000 galaxies (19766.0 gal/s, 0.1s for last 1000, 4476 pairs so far)\n",
      "  Chunk [190000-200000]: Processed 7000/10000 galaxies (15247.9 gal/s, 0.1s for last 1000, 5161 pairs so far)\n",
      "  Chunk [190000-200000]: Processed 8000/10000 galaxies (22033.2 gal/s, 0.0s for last 1000, 5808 pairs so far)\n",
      "  Chunk [190000-200000]: Processed 9000/10000 galaxies (27874.7 gal/s, 0.0s for last 1000, 6408 pairs so far)\n",
      "  Chunk [190000-200000] COMPLETE: 10000 galaxies in 0.5s, found 1717 pairs\n",
      "\n",
      "  Checkpoint: Saved 88508 pairs to data/paircatalogs/galaxy_pairs_catalog_CMASS_South_10_9_11hmpc_temp_20.csv\n",
      "  Chunk [200000-210000]: Processed 1000/10000 galaxies (27794.7 gal/s, 0.0s for last 1000, 592 pairs so far)\n",
      "  Chunk [200000-210000]: Processed 2000/10000 galaxies (33588.6 gal/s, 0.0s for last 1000, 1143 pairs so far)\n",
      "  Chunk [200000-210000]: Processed 3000/10000 galaxies (28315.8 gal/s, 0.0s for last 1000, 1618 pairs so far)\n",
      "  Chunk [200000-210000]: Processed 4000/10000 galaxies (29656.0 gal/s, 0.0s for last 1000, 2133 pairs so far)\n",
      "  Chunk [200000-210000]: Processed 5000/10000 galaxies (34372.5 gal/s, 0.0s for last 1000, 2625 pairs so far)\n",
      "  Chunk [200000-210000]: Processed 6000/10000 galaxies (36283.2 gal/s, 0.0s for last 1000, 3080 pairs so far)\n",
      "  Chunk [200000-210000]: Processed 7000/10000 galaxies (28647.7 gal/s, 0.0s for last 1000, 3509 pairs so far)\n",
      "  Chunk [200000-210000]: Processed 8000/10000 galaxies (38520.5 gal/s, 0.0s for last 1000, 3933 pairs so far)\n",
      "  Chunk [200000-210000]: Processed 9000/10000 galaxies (39836.1 gal/s, 0.0s for last 1000, 4351 pairs so far)\n",
      "  Chunk [200000-210000] COMPLETE: 10000 galaxies in 0.3s, found 1172 pairs\n",
      "  Chunk [210000-213205]: Processed 1000/3205 galaxies (36276.6 gal/s, 0.0s for last 1000, 357 pairs so far)\n",
      "  Chunk [210000-213205]: Processed 2000/3205 galaxies (21353.9 gal/s, 0.0s for last 1000, 678 pairs so far)\n",
      "  Chunk [210000-213205]: Processed 3000/3205 galaxies (45468.7 gal/s, 0.0s for last 1000, 880 pairs so far)\n",
      "  Chunk [210000-213205] COMPLETE: 3205 galaxies in 0.1s, found 214 pairs\n",
      "\n",
      "Found 89894 total pairs\n",
      "Saved to data/paircatalogs/galaxy_pairs_catalog_CMASS_South_10_9_11hmpc.csv\n",
      "Total valid pairs: 89894\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
